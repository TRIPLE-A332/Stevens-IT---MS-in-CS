{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089a825b",
   "metadata": {},
   "source": [
    "# Q3. Bank Churn Classification Problem\n",
    "## Dataset Description:\n",
    "Banking is one of those traditional industries that has gone through a steady transformation over the past few decades. Yet, many banks today with a sizeable customer base are hoping to gain a competitive edge but have not tapped into the vast amounts of data they have, especially in solving one of the most acknowledged problems – customer churn (i.e., a customer leaving the bank). It is advantageous to banks to know what leads a client to leave the bank. Banks often use the customer churn rate as one of their key business metrics because the cost of retaining existing customers is far less than acquiring new ones, and meanwhile increasing customer retention can greatly increase profits.  \n",
    "Churn prevention allows companies to develop different programs such as loyalty and retention programs to keep as many customers as possible. Following are the attributes of the dataset we will be working with. \n",
    " \n",
    "\n",
    "- RowNumber (continuous) — corresponds to the record (row) number and has no effect on the output. \n",
    " \n",
    "\n",
    "- CustomerId  (categorical)— contains random values and has no effect on customer leaving the bank. \n",
    " \n",
    "\n",
    "- Surname  (categorical)— the surname of a customer has no impact on their decision to leave the bank \n",
    " \n",
    "\n",
    "- CreditScore  (continuous) — can influence customer churn, since a customer with a higher credit score is less likely to leave the bank. \n",
    " \n",
    "\n",
    "- Geography (categorical) — a customer’s location can affect their decision to leave the bank. \n",
    " \n",
    "\n",
    "- Gender (categorical) — it’s interesting to explore whether gender plays a role in a customer leaving the bank. \n",
    " \n",
    "\n",
    "- Age (continuous) — this is certainly relevant, since older customers are less likely to leave their bank than younger ones. \n",
    " \n",
    "\n",
    "- Tenure (continuous) — refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank. \n",
    " \n",
    "\n",
    "- Balance (continuous) — also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances. \n",
    " \n",
    "\n",
    "- NumOfProducts (continuous) — refers to the number of products that a customer has purchased through the bank. \n",
    " \n",
    "\n",
    "- HasCrCard (categorical) — denotes whether a customer has a credit card. This column is also relevant since people with a credit card are less likely to leave the bank. \n",
    " \n",
    "\n",
    "- IsActiveMember (categorical) — active customers are less likely to leave the bank. \n",
    " \n",
    "\n",
    "- EstimatedSalary (continuous) — as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries. \n",
    " \n",
    "\n",
    "- Exited (Categorical) — whether or not the customer left the bank. (Target variable) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a4322",
   "metadata": {},
   "source": [
    "# Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b45d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Scaling & splitting Libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Sampling library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Evaluation Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7cc412",
   "metadata": {},
   "source": [
    "### b.\tData Loading / Preprocessing\n",
    "#### i.\tLoading\n",
    "1. Load the data <BankChurn.csv> as a pandas dataframe using the `pd.read_csv()` function which returns a dataframe , store this value in a variable named ‘df’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb570e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c024de29",
   "metadata": {},
   "source": [
    "2. The resulting dataframe should have the shape (10000,14) indicating that there are 10000 instances and 14 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97260c2d",
   "metadata": {},
   "source": [
    "3. In this dataframe, currently you have 9 features which are the following: RowNumber, CustomerID, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary. Using the ‘pandas.dataframe.drop’ function to drop the RowNumber, CustomerID and Surname columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e2c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6ea1c2",
   "metadata": {},
   "source": [
    "4. Using the ‘pandas.isnull()’ function check if there are any missing values in the dataframe and report this value (i.e., the number of missing values per column of the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42f66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26d315ee",
   "metadata": {},
   "source": [
    "5. Your task is to use feature columns to predict the target column (which is categorical in our case). This can be cast as a classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24eeb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5de148fe",
   "metadata": {},
   "source": [
    "6. Create a dataframe X of features (by dropping the ‘Exited’ column from the original dataframe). Create a Pandas Series object of targets Y (by only considering the ‘Exited’ column from the original dataframe). Moving forward, we will be working with X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aff754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ed0acb0",
   "metadata": {},
   "source": [
    "### ii. Data Visualisation\n",
    "1. Visualize the distribution of the ‘Age’ and ‘CreditScore’ column using the ‘matplotlib.pyplot.hist’ function as two separate plots. Label the x-axis and the y-axis along with giving the plot a title and assign a bin size of 7.\n",
    "\n",
    "- What are the respective mean values of these two features (use the pandas.DataFrame.mean() function)?\n",
    "- What is the respective standard deviation of these two features (use the pandas.DataFrame.std() function)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd629fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47b86ad8",
   "metadata": {},
   "source": [
    "2. Only for this question use the dataframe consisting of the target variable (initialized as ‘df’). Using matplotlib visualize the number of males and females in each country who are active members and not active members. (Visualize this using a barchart. You will need to use the ‘Gender’, ‘Geography’ and ‘IsActiveMember’ features for this question). Visualize these graphs on two separate plots with respect to their active status. To create a barchart using matplotlib use the ‘matplotlib.pyplot.bar()’ function. Also label the x-axis, y-axis and give the plots a title. \n",
    "\n",
    "- How many males are from France and are active members?\n",
    "- How many females are from Spain and are active members?\n",
    "- How many males are from France or Germany who are not active members?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb2554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5316b3bf",
   "metadata": {},
   "source": [
    "3. Using the target variable in Y plot a bar chart showing the distribution of the ‘Exited’ column (To create a barchart using matplotlib use the ‘matplotlib.pyplot.bar()’ function). \n",
    "\n",
    "- What can be said about this distribution (specifically keeping in mind this distribution represents the target variable) will this have an impact on the results of the classification model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05821c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a466cd5d",
   "metadata": {},
   "source": [
    "4. So far you should have successfully been able to load, preprocess and visualize your data. Now, use the ‘pd.get_dummies()’ function to convert categorical data into dummy variables (‘Gender’ and ‘Geography’).\n",
    "**(Perform this only on X)**. \n",
    "\n",
    "- What is the shape of X?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70122b33",
   "metadata": {},
   "source": [
    "### iii. Data Splitting\n",
    "1. Split data into training and test sets using the sklearn ‘train_test_split() function in a **80:20** ratio. The result of your data split should be X_train, X_test, y_train, y_test. (Respectively your training features, testing features, training targets and testing target arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0117349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "273dcd1e",
   "metadata": {},
   "source": [
    "### iv. Data Scaling\n",
    "1. Employ the ‘MinMaxScaler’  function on the continuous attributes in X_train. Employ the ‘fit_transform()’ function of the scaler to retrieve the new (scaled) version of the training data (i.e., fit_transform() should be run on `X_train`). Store the result in X_train again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409aa2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02859b72",
   "metadata": {},
   "source": [
    "2. Scale the X_test data using the scaler you have just fit, this time using the `transform()` function. Note: store the scaled values back into X_test.  At the end of this step, you must have X_train, X_test, scaled according to the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d897f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a07196d",
   "metadata": {},
   "source": [
    "### c. Modelling\n",
    "### i. Modeling (Model Instantiation / Training) using Logistic Regression classifier \n",
    "1. Employ the Logistic Regression classifier from sklearn and instantiate the model. Label this model as ‘model_1_lr’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fe862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33263994",
   "metadata": {},
   "source": [
    "2. Once instantiated, `fit()` the model using the scaled X_train, y_train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017fe9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94b94480",
   "metadata": {},
   "source": [
    "3. Employ the `predict()` function to obtain predictions on X_test and store this in a variable labeled as ‘y_pred_lr’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ba92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e2929e7",
   "metadata": {},
   "source": [
    "4. Employ the ‘accuracy_score()’ function by using the ‘y_pred_lr’ and ‘y_test’ variables as the functions parameters and print the accuracy of the Logistic Regression model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbd18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2503ef4",
   "metadata": {},
   "source": [
    "### ii. Modeling (Model Instantiation / Training) using Support Vector Machine Classifier \n",
    "\n",
    "1. Employ the Support Vector Machine (SVM) classifier from sklearn and instantiate the model. Label this model as ‘model_2_svm’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd49f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b73401",
   "metadata": {},
   "source": [
    "2. Once instantiated, ‘fit()’ the model using the scaled X_train, y_train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d91f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d29081",
   "metadata": {},
   "source": [
    "3. Employ the ‘predict()’ function to obtain predictions on X_test and store this in a variable labeled as ‘y_pred_svm’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9de54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c736671f",
   "metadata": {},
   "source": [
    "4. Employ the ‘accuracy_score’ function (‘sklearn.metrics.accuracy()’ function) by using the ‘y_pred_lr’ and ‘y_test’ variables as the functions parameters and print the accuracy of the SVM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d9c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "440edeea",
   "metadata": {},
   "source": [
    "### iii. Modeling Logistic Regression Classifier on a balanced dataset \n",
    "1. Employ Synthetic Minority Oversampling on X_train and y_train. To use SMOTE you will have to install the imbalanced-learn library, this can either be down by executing the following command ‘pip install -U imbalanced-learn’ command ‘conda install -c conda-forage imbalanced-learn’ command for the Anaconda Cloud platform. (For more information click the following link: https://imbalanced-learn.org/stable/install.html).  \n",
    "Import the ‘SMOTE’ function from the ‘imblearn.over_sampling’. Use the ‘smote.refit_resample()’ function on X_train and y_train using its default parameters. Store them in X_train_smote, y_train_smote. - Be careful to employ SMOTE ONLY on the training data and not on the full dataset because that can cause inadvertent “data leakage” (please see: https://arxiv.org/pdf/2107.00079.pdf for details) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03f658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d0e8304",
   "metadata": {},
   "source": [
    "2. Employ a new Logistic Regression classifier from sklearn and instantiate the model. Label this model as ‘model_3_smote_lr’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fba81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35977358",
   "metadata": {},
   "source": [
    "3. Once instantiated, ‘fit()’ the model using the balanced X_train_smote, y_train_smote data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21c2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e40a199c",
   "metadata": {},
   "source": [
    "4. Employ the ‘predict()’ function to obtain predictions on X_test and store this in a variable labeled as ‘y_pred_smote_lr’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7a4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "488da2f5",
   "metadata": {},
   "source": [
    "5. Employ the ‘accuracy_score’ function by using the ‘y_pred_lr’ and ‘y_test’ variables as the functions parameters and print the accuracy of the new Logistic Regression model.\n",
    "\n",
    "- What is your initial observation of the accuracy of model_3_smote_lr vs. accuracy of model_1_lr? What could be the reasoning for (any possible) change in accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912eb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c09c15f",
   "metadata": {},
   "source": [
    "### iv. Modeling SVM on a balanced dataset\n",
    "1. Employ Synthetic Minority Oversampling on X_train and y_train. Import the ‘SMOTE’ function from the ‘imblearn.over_sampling’. Use the ‘smote.refit_resample()’ function on X_train and y_train. Store them in X_train_smote, y_train_smote. \n",
    "\n",
    "- At the end of this step, your new training set i.e., (X_train_smote , y_train_smote) should have the same number of instances for each of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e7f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb819ddf",
   "metadata": {},
   "source": [
    "2. Employ a new SVM classifier from sklearn and instantiate the model. Label this model as ‘model_4_smote_svm’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a246b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f243689",
   "metadata": {},
   "source": [
    "3. Once instantiated, ‘fit()’ the model using the balanced X_train_smote, y_train_smote data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9d059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d693022",
   "metadata": {},
   "source": [
    "4. Employ the ‘predict()’ function to obtain predictions on X_test and store this in a variable labeled as ‘y_pred_smote_svm’.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2d12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcdc3bdf",
   "metadata": {},
   "source": [
    "5. Employ the ‘accuracy_score’ function (‘sklearn.metrics.accuracy()’ function) by using the ‘y_pred_lr’ and ‘y_test’ variables as the functions parameters and print the accuracy of the new SVM model. \n",
    "\n",
    "- What is your initial observation of the accuracy of model_4_smote_svm vs. accuracy of model_2_svm? What could be the reasoning for (any possible) change in accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a63962",
   "metadata": {},
   "source": [
    "### Modeling Grid Search Parameter Selection for SVM \n",
    "1. We will now be reverting to our X_train and y_train data. Initialize a variable labeled as ‘param_grid’ storing the following: {\"gamma\": [0.001, 0.01, 0.1], \"C\": [1,10,100,1000,10000]}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aee4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0129977f",
   "metadata": {},
   "source": [
    "2. Employ the gridsearchCV function and initialize the following parameters: estimator = SVC(), param_grid = param_grid, cv=5, verbose =1, scoring = ‘accuracy’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ee1def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc284419",
   "metadata": {},
   "source": [
    "3.  Once instantiated, ‘fit()’ the model using the X_train_smote, y_train_smote data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697456c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6537ff",
   "metadata": {},
   "source": [
    "4. Print the best paramaters using the **‘best_params_’** attribute and print the mean cross validated score of the best estimator (hint use the ‘best_score_’ attribute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759b578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7d0136b",
   "metadata": {},
   "source": [
    "5. Employ the ‘score’ function by using the ‘X_test’ and ‘y_test’ variables as the functions parameters and print the accuracy of the new gridsearch SVM model.SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3c0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c972f8",
   "metadata": {},
   "source": [
    "### d. Evaluation\n",
    "1. (2 points) Calculate F1 Score, Precision, Recall, Accuracy (All on the test set X_test, y_test) .\n",
    "\n",
    "- Employ the `classification_report()` function from sklearn.metrics to report the precision recall, f1 score and accuracy for each class for the first **four models (parts c.i – c.iv).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd484c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a2aad9",
   "metadata": {},
   "source": [
    "2. Visualize a confusion matrix for the first four models \n",
    "\n",
    "- Employ the `confusion_matrix()` function from sklearn.metrics to report the confusion matrix results.\n",
    "- Report the False Negative and False Positive values for model_1_lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf9706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6377f6df",
   "metadata": {},
   "source": [
    "3. Report the best F1 score of the grid search implemented in the fifth model **(part c.v)**. Also report the best parameters from the grid search on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6308c54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
